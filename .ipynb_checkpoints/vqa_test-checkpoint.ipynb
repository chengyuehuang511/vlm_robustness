{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6674f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 14:41:00.769747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 14:41:02.330239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import lavis.tasks as tasks\n",
    "from lavis.common.config import Config\n",
    "from lavis.common.dist_utils import get_rank, init_distributed_mode\n",
    "from lavis.common.logger import setup_logger\n",
    "from lavis.common.optims import (\n",
    "    LinearWarmupCosineLRScheduler,\n",
    "    LinearWarmupStepLRScheduler,\n",
    ")\n",
    "from lavis.common.utils import now\n",
    "\n",
    "# imports modules for registration\n",
    "from lavis.datasets.builders import *\n",
    "from data.builders import *\n",
    "from model import *\n",
    "from lavis.models import *\n",
    "from lavis.processors import *\n",
    "from lavis.runners.runner_base import RunnerBase\n",
    "from lavis.tasks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4771c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed, deterministic=False):\n",
    "    \"\"\"Set random seed.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed to be used.\n",
    "        deterministic (bool): Whether to set the deterministic option for\n",
    "            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`\n",
    "            to True and `torch.backends.cudnn.benchmark` to False.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423e37e",
   "metadata": {},
   "source": [
    "# Albef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1573ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 12:31:49,267 - base_model - Missing keys []\n",
      "INFO - 2024-07-01 12:31:49,268 - base_model - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/ALBEF/albef_vqav2_lavis.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(7.4633, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 384, 384).to(\"cuda:0\")\n",
    "model = load_model(\"albef_vqa\", model_type=\"vqav2\").to(\"cuda:0\")\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"cat\", \"dog\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.25, 0.25, 0.25, 0.25, 1.0]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([4, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e8998da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 13:39:07,171 - base_model - Missing keys []\n",
      "INFO - 2024-07-01 13:39:07,172 - base_model - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/ALBEF/albef_vqav2_lavis.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(7.4977, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "output_new.loss:  tensor(7.4977, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 384, 384).to(\"cuda:0\")\n",
    "model = load_model(\"albef_vqa\", model_type=\"vqav2\").to(\"cuda:0\").eval()\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5, 0.5, 1.0]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([2, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output.loss)\n",
    "\n",
    "samples_new = {\n",
    "    \"image\": torch.stack([image_rand[0], image_rand[0], image_rand[1]]),\n",
    "    \"text_input\": [\"What is this?\", \"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5 * 3/2, 0.5 * 3/2, 1.0 * 3/2]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1, 1, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output_new = model(samples_new)\n",
    "print(\"output_new.loss: \", output_new.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e80b899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2054, 2003, 2023, 1029,  102],\n",
       "        [ 101, 2054, 2003, 2023, 1029,  102],\n",
       "        [ 101, 2054, 2003, 2008, 1029,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = model.tokenizer(\n",
    "    [\"What is this?\", \"What is this?\", \"What is that?\"],\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=1000,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18beb44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2054, 2003, 2023, 1029,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_one = model.tokenizer(\n",
    "    [\"What is this?\"],\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=1000,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "tokenized_text_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97d8a059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1084,  0.1659,  0.2482,  ..., -0.4964, -0.0917,  0.2431],\n",
       "         [ 0.4797,  0.4930,  0.3895,  ..., -0.7556, -0.4877,  0.5836],\n",
       "         [ 1.0337,  0.6387,  0.9262,  ..., -0.4521, -0.3721, -0.4231],\n",
       "         ...,\n",
       "         [ 0.5401,  0.1490,  0.0636,  ..., -1.2835,  0.4325, -0.9287],\n",
       "         [ 0.0058,  0.1974, -0.5898,  ..., -0.8813,  0.2463,  0.0333],\n",
       "         [-0.0853,  0.0534, -0.6248,  ..., -1.1418,  0.4026,  0.0579]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.intermediate_output.image_embeds[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f50a99e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1084,  0.1659,  0.2482,  ..., -0.4964, -0.0917,  0.2431],\n",
       "         [ 0.4797,  0.4930,  0.3895,  ..., -0.7556, -0.4877,  0.5836],\n",
       "         [ 1.0337,  0.6387,  0.9262,  ..., -0.4521, -0.3721, -0.4231],\n",
       "         ...,\n",
       "         [ 0.5401,  0.1490,  0.0636,  ..., -1.2835,  0.4325, -0.9287],\n",
       "         [ 0.0058,  0.1974, -0.5898,  ..., -0.8813,  0.2463,  0.0333],\n",
       "         [-0.0853,  0.0534, -0.6248,  ..., -1.1418,  0.4026,  0.0579]],\n",
       "\n",
       "        [[-0.1084,  0.1659,  0.2482,  ..., -0.4964, -0.0917,  0.2431],\n",
       "         [ 0.4797,  0.4930,  0.3895,  ..., -0.7556, -0.4877,  0.5836],\n",
       "         [ 1.0337,  0.6387,  0.9262,  ..., -0.4521, -0.3721, -0.4231],\n",
       "         ...,\n",
       "         [ 0.5401,  0.1490,  0.0636,  ..., -1.2835,  0.4325, -0.9287],\n",
       "         [ 0.0058,  0.1974, -0.5898,  ..., -0.8813,  0.2463,  0.0333],\n",
       "         [-0.0853,  0.0534, -0.6248,  ..., -1.1418,  0.4026,  0.0579]],\n",
       "\n",
       "        [[-0.0857,  0.0592,  0.2299,  ..., -0.5149, -0.0368,  0.2147],\n",
       "         [ 0.5761,  0.4005,  0.2458,  ..., -0.3781, -0.2357,  0.0234],\n",
       "         [ 1.0358,  0.1324,  0.4406,  ..., -1.0334, -0.1554, -0.3193],\n",
       "         ...,\n",
       "         [-0.4491,  0.0153, -0.3039,  ..., -1.0997,  0.5476,  0.0169],\n",
       "         [-0.4390,  0.0130, -0.4015,  ..., -1.1470,  0.4240,  0.0189],\n",
       "         [-0.1121,  0.2486, -0.4720,  ..., -0.7873,  0.6209,  0.2727]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.intermediate_output.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ded4381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 577, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.intermediate_output.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98e41d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0211,  0.1497,  0.3861,  ..., -0.3361,  0.0171, -0.4375],\n",
       "         [-0.0165,  0.2702, -0.2013,  ...,  0.1000,  0.1935, -0.0024],\n",
       "         [ 0.1865,  0.1759,  0.0120,  ...,  0.1826,  0.3152,  0.2304],\n",
       "         [ 0.1856,  0.1138,  0.0652,  ..., -0.0163,  0.2820,  0.1953],\n",
       "         [-0.3360,  0.0219, -0.2263,  ...,  0.0550,  0.2021, -0.0053],\n",
       "         [-0.2757,  0.0398, -0.2054,  ...,  0.0777,  0.2185,  0.0145]],\n",
       "\n",
       "        [[-0.0211,  0.1497,  0.3861,  ..., -0.3361,  0.0171, -0.4375],\n",
       "         [-0.0165,  0.2702, -0.2013,  ...,  0.1000,  0.1935, -0.0024],\n",
       "         [ 0.1865,  0.1759,  0.0120,  ...,  0.1826,  0.3152,  0.2304],\n",
       "         [ 0.1856,  0.1138,  0.0652,  ..., -0.0163,  0.2820,  0.1953],\n",
       "         [-0.3360,  0.0219, -0.2263,  ...,  0.0550,  0.2021, -0.0053],\n",
       "         [-0.2757,  0.0398, -0.2054,  ...,  0.0777,  0.2185,  0.0145]],\n",
       "\n",
       "        [[ 0.0860,  0.2904,  0.4131,  ..., -0.3347,  0.0586, -0.5755],\n",
       "         [ 0.0639,  0.1792, -0.1922,  ..., -0.1420,  0.1322, -0.0664],\n",
       "         [ 0.2262,  0.1518, -0.1012,  ..., -0.0654,  0.2100,  0.1151],\n",
       "         [ 0.1013,  0.0876, -0.0941,  ..., -0.1563,  0.2485,  0.1559],\n",
       "         [-0.3125,  0.0323, -0.2332,  ...,  0.0400,  0.2050, -0.0297],\n",
       "         [-0.2555,  0.0622, -0.2154,  ...,  0.0544,  0.2151,  0.0068]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "model.text_encoder.forward_automask(\n",
    "    tokenized_text=tokenized_text, visual_embeds=output_new.intermediate_output.image_embeds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9e3ec6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0211,  0.1497,  0.3861,  ..., -0.3361,  0.0171, -0.4375],\n",
       "         [-0.0165,  0.2702, -0.2013,  ...,  0.1000,  0.1935, -0.0024],\n",
       "         [ 0.1865,  0.1759,  0.0120,  ...,  0.1826,  0.3152,  0.2304],\n",
       "         [ 0.1856,  0.1138,  0.0652,  ..., -0.0163,  0.2820,  0.1953],\n",
       "         [-0.3360,  0.0219, -0.2263,  ...,  0.0550,  0.2021, -0.0053],\n",
       "         [-0.2757,  0.0398, -0.2054,  ...,  0.0777,  0.2185,  0.0145]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "model.text_encoder.forward_automask(\n",
    "    tokenized_text=tokenized_text_one, visual_embeds=output_new.intermediate_output.image_embeds[[1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff47ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['image_embeds', 'image_embeds_m', 'encoder_output', 'encoder_output_m', 'decoder_output', 'decoder_labels'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.intermediate_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cdec9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0211,  0.1497,  0.3861,  ..., -0.3361,  0.0171, -0.4375],\n",
       "         [-0.0165,  0.2702, -0.2013,  ...,  0.1000,  0.1935, -0.0024],\n",
       "         [ 0.1865,  0.1759,  0.0120,  ...,  0.1826,  0.3152,  0.2304],\n",
       "         [ 0.1856,  0.1138,  0.0652,  ..., -0.0163,  0.2820,  0.1953],\n",
       "         [-0.3360,  0.0219, -0.2263,  ...,  0.0550,  0.2021, -0.0053],\n",
       "         [-0.2757,  0.0398, -0.2054,  ...,  0.0777,  0.2185,  0.0145]],\n",
       "\n",
       "        [[ 0.0860,  0.2904,  0.4131,  ..., -0.3347,  0.0586, -0.5755],\n",
       "         [ 0.0639,  0.1792, -0.1922,  ..., -0.1420,  0.1322, -0.0664],\n",
       "         [ 0.2262,  0.1518, -0.1012,  ..., -0.0654,  0.2100,  0.1151],\n",
       "         [ 0.1013,  0.0876, -0.0941,  ..., -0.1563,  0.2485,  0.1559],\n",
       "         [-0.3125,  0.0323, -0.2332,  ...,  0.0400,  0.2050, -0.0297],\n",
       "         [-0.2555,  0.0622, -0.2154,  ...,  0.0544,  0.2151,  0.0068]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.intermediate_output.encoder_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c603dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0211,  0.1497,  0.3861,  ..., -0.3361,  0.0171, -0.4375],\n",
       "         [-0.0165,  0.2702, -0.2013,  ...,  0.1000,  0.1935, -0.0024],\n",
       "         [ 0.1865,  0.1759,  0.0120,  ...,  0.1826,  0.3152,  0.2304],\n",
       "         [ 0.1856,  0.1138,  0.0652,  ..., -0.0163,  0.2820,  0.1953],\n",
       "         [-0.3360,  0.0219, -0.2263,  ...,  0.0550,  0.2021, -0.0053],\n",
       "         [-0.2757,  0.0398, -0.2054,  ...,  0.0777,  0.2185,  0.0145]],\n",
       "\n",
       "        [[-0.0211,  0.1497,  0.3861,  ..., -0.3361,  0.0171, -0.4375],\n",
       "         [-0.0165,  0.2702, -0.2013,  ...,  0.1000,  0.1935, -0.0024],\n",
       "         [ 0.1865,  0.1759,  0.0120,  ...,  0.1826,  0.3152,  0.2304],\n",
       "         [ 0.1856,  0.1138,  0.0652,  ..., -0.0163,  0.2820,  0.1953],\n",
       "         [-0.3360,  0.0219, -0.2263,  ...,  0.0550,  0.2021, -0.0053],\n",
       "         [-0.2757,  0.0398, -0.2054,  ...,  0.0777,  0.2185,  0.0145]],\n",
       "\n",
       "        [[ 0.0860,  0.2904,  0.4131,  ..., -0.3347,  0.0586, -0.5755],\n",
       "         [ 0.0639,  0.1792, -0.1922,  ..., -0.1420,  0.1322, -0.0664],\n",
       "         [ 0.2262,  0.1518, -0.1012,  ..., -0.0654,  0.2100,  0.1151],\n",
       "         [ 0.1013,  0.0876, -0.0941,  ..., -0.1563,  0.2485,  0.1559],\n",
       "         [-0.3125,  0.0323, -0.2332,  ...,  0.0400,  0.2050, -0.0297],\n",
       "         [-0.2555,  0.0622, -0.2154,  ...,  0.0544,  0.2151,  0.0068]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.intermediate_output.encoder_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49990b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1774a8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.3261, -5.1228, -5.1128,  ..., -5.0842, -5.0993, -5.0917],\n",
       "         [-7.4742, -7.7043, -7.6620,  ..., -7.6386, -7.7235, -7.6797],\n",
       "         [-2.5261, -2.5918, -2.5996,  ..., -2.5963, -2.5992, -2.5989]],\n",
       "\n",
       "        [[-5.3261, -5.1228, -5.1128,  ..., -5.0842, -5.0993, -5.0917],\n",
       "         [-7.3359, -7.6161, -7.5978,  ..., -7.5692, -7.6225, -7.6111],\n",
       "         [-2.5266, -2.5933, -2.6008,  ..., -2.5978, -2.6008, -2.6003]],\n",
       "\n",
       "        [[-5.4162, -5.2113, -5.2096,  ..., -5.1702, -5.1884, -5.1868],\n",
       "         [-7.3218, -7.6313, -7.6345,  ..., -7.6032, -7.6584, -7.6454],\n",
       "         [-2.5435, -2.6079, -2.6154,  ..., -2.6123, -2.6143, -2.6142]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.intermediate_output.decoder_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b8f4b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.3667, 8.0175, 6.8032], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.intermediate_output.decoder_output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a808dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4977, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output.intermediate_output.decoder_output.loss * torch.tensor([0.5, 0.5, 1.0]).to(\"cuda:0\")) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7df7476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4977, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5564e015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.3261, -5.1228, -5.1128,  ..., -5.0842, -5.0993, -5.0917],\n",
       "         [-7.4742, -7.7043, -7.6620,  ..., -7.6386, -7.7235, -7.6797],\n",
       "         [-2.5261, -2.5918, -2.5996,  ..., -2.5963, -2.5992, -2.5989]],\n",
       "\n",
       "        [[-5.3261, -5.1228, -5.1128,  ..., -5.0842, -5.0993, -5.0917],\n",
       "         [-7.3359, -7.6161, -7.5978,  ..., -7.5692, -7.6226, -7.6111],\n",
       "         [-2.5266, -2.5933, -2.6008,  ..., -2.5978, -2.6008, -2.6003]],\n",
       "\n",
       "        [[-5.4162, -5.2113, -5.2096,  ..., -5.1702, -5.1884, -5.1868],\n",
       "         [-7.3218, -7.6314, -7.6345,  ..., -7.6032, -7.6584, -7.6454],\n",
       "         [-2.5435, -2.6079, -2.6154,  ..., -2.6123, -2.6143, -2.6142]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.intermediate_output.decoder_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84d5ee28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.3667, 8.0175, 6.8032], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.intermediate_output.decoder_output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfc23d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9984, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output.intermediate_output.decoder_output.loss * torch.tensor([0.5, 0.5, 1.0]).to(\"cuda:0\")) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ede79eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9984, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2f4f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 13:47:17,642 - base_model - Missing keys []\n",
      "INFO - 2024-07-01 13:47:17,643 - base_model - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/ALBEF/albef_vqav2_lavis.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(8.1921, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "output_new.loss:  tensor(8.1921, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(1, 3, 384, 384).to(\"cuda:0\")\n",
    "model = load_model(\"albef_vqa\", model_type=\"vqav2\").to(\"cuda:0\").eval()\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"answer\": [\"cat\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5, 0.5]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([2]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output.loss)\n",
    "\n",
    "samples_new = {\n",
    "    \"image\": torch.stack([image_rand[0], image_rand[0]]),\n",
    "    \"text_input\": [\"What is this?\", \"What is this?\"],\n",
    "    \"answer\": [\"cat\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5 * 2, 0.5 * 2]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output_new = model(samples_new)\n",
    "print(\"output_new.loss: \", output_new.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c44dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 13:48:01,359 - base_model - Missing keys []\n",
      "INFO - 2024-07-01 13:48:01,360 - base_model - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/ALBEF/albef_vqav2_lavis.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(7.5849, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "output_new.loss:  tensor(7.5849, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 384, 384).to(\"cuda:0\")\n",
    "model = load_model(\"albef_vqa\", model_type=\"vqav2\").to(\"cuda:0\").eval()\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"cat\", \"cat\", \"cat\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.25, 0.25, 0.25, 0.25, 1.0]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([4, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output.loss)\n",
    "\n",
    "samples_new = {\n",
    "    \"image\": torch.stack([image_rand[0], image_rand[0], image_rand[0], image_rand[0], image_rand[1]]),\n",
    "    \"text_input\": [\"What is this?\", \"What is this?\", \"What is this?\", \"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"cat\", \"cat\", \"cat\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.25 * 5/2, 0.25 * 5/2, 0.25 * 5/2, 0.25 * 5/2, 1.0 * 5/2]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1, 1, 1, 1, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output_new = model(samples_new)\n",
    "print(\"output_new.loss: \", output_new.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08ba64d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(sum(sum(torch.stack([image_rand[0], image_rand[1]]) != image_rand))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c48d6a",
   "metadata": {},
   "source": [
    "# Blip2_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5475c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 14:08:45,683 - blip2_t5_vqa - freeze vision encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9632d4f26fd94e05b6b7be532e20f286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 14:08:54,545 - blip2 - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(6.3660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output_new.loss:  tensor(6.3660, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "model = load_model(\"blip2_t5_vqa\", model_type=\"pretrain_flant5xl\").to(\"cuda:0\").eval()\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5, 0.5, 1.0]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([2, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])\n",
    "\n",
    "samples_new = {\n",
    "    \"image\": torch.stack([image_rand[0], image_rand[0], image_rand[1]]),\n",
    "    \"text_input\": [\"What is this?\", \"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5 * 3/2, 0.5 * 3/2, 1.0 * 3/2]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1, 1, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output_new = model(samples_new)\n",
    "print(\"output_new.loss: \", output_new[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8040d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 15:55:52,813 - blip2_t5_vqa - freeze vision encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0c4cd2148d43f0953b6fbf1594dc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 15:56:03,790 - blip2 - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(5.9060, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand[[0]],\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"answer\": [\"cat\"],\n",
    "    \"weight\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"blip2_t5_vqa\", model_type=\"pretrain_flant5xl\").to(\"cuda:0\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c109b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 15:56:30,011 - blip2_t5 - freeze vision encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb526f93c3bc4ed19ae6d270bb15a75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 15:56:39,585 - blip2 - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_blip2_t5.loss:  tensor(5.9062, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand[[0]],\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"text_output\": [\"cat\"],\n",
    "    \"weight\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model_blip2_t5 = load_model(\"blip2_t5\", model_type=\"pretrain_flant5xl\").to(\"cuda:0\").eval()\n",
    "output_blip2_t5 = model_blip2_t5(samples)\n",
    "print(\"output_blip2_t5.loss: \", output_blip2_t5[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb9578",
   "metadata": {},
   "source": [
    "# Blip2_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71c3a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 16:23:05,968 - blip2_opt_vqa - freeze vision encoder\n",
      "INFO - 2024-07-01 16:23:10,665 - blip2 - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(12.1484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output_new.loss:  tensor(12.1471, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "model = load_model(\"blip2_opt_vqa\", model_type=\"pretrain_opt2.7b\").to(\"cuda:0\").eval()\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5, 0.5, 1.0]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([2, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])\n",
    "\n",
    "samples_new = {\n",
    "    \"image\": torch.stack([image_rand[0], image_rand[0], image_rand[1]]),\n",
    "    \"text_input\": [\"What is this?\", \"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5 * 3/2, 0.5 * 3/2, 1.0 * 3/2]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1, 1, 1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "output_new = model(samples_new)\n",
    "print(\"output_new.loss: \", output_new[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2136e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 16:03:44,197 - blip2_opt_vqa - freeze vision encoder\n",
      "INFO - 2024-07-01 16:03:48,855 - blip2 - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(9.8770, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand[[0]],\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"answer\": [\"cat\"],\n",
    "    \"weight\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"blip2_opt_vqa\", model_type=\"pretrain_opt2.7b\").to(\"cuda:0\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42fc908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2024-07-01 16:06:54,926 - blip2_opt - freeze vision encoder\n",
      "INFO - 2024-07-01 16:06:59,216 - blip2 - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_blip2_opt.loss:  tensor(3.3452, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand[[0]],\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"text_output\": [\"cat\"],\n",
    "    \"weight\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model_blip2_opt = load_model(\"blip2_opt\", model_type=\"pretrain_opt2.7b\").to(\"cuda:0\").eval()\n",
    "output_blip2_opt = model_blip2_opt(samples)\n",
    "print(\"output_blip2_opt.loss: \", output_blip2_opt[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee045e",
   "metadata": {},
   "source": [
    "# PaliGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbbfa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686d7b06dffe4e35b8d293da852c98cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions ['What is this?']\n",
      "answers ['cat']\n",
      "weight tensor([1], device='cuda:0')\n",
      "n_answers tensor([1], device='cuda:0')\n",
      "output.loss:  tensor(3.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand[[0]],\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"answer\": [\"cat\"],\n",
    "    \"weight\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"paligemma_vqa\", model_type=\"paligemma-3b-ft-vqav2-448\").to(\"cuda:0\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e75068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaliGemmaCausalLMOutputWithPast(loss=tensor(3.6730, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-1.5483, 10.9163, -8.9859,  ..., -2.1964, -2.2345, -2.2782],\n",
       "         [-7.8914, 12.2948, -8.2089,  ..., -5.5079, -5.5658, -5.6053],\n",
       "         [-9.0410, 11.1091,  1.7776,  ..., -6.8556, -6.9023, -6.9447],\n",
       "         ...,\n",
       "         [ 4.0500, 13.8408,  3.0179,  ...,  3.9606,  3.8709,  3.8631],\n",
       "         [-8.9864, 16.6010, -6.0495,  ..., -6.8220, -6.8987, -6.9506],\n",
       "         [-7.3626, 16.0273, -6.2692,  ..., -6.3612, -6.4248, -6.4581]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 2.7882e+00,  4.7940e+00,  5.4377e+00,  ..., -2.5283e+00,\n",
       "           -1.5273e+00,  2.1634e+00],\n",
       "          [ 2.5082e+00,  1.1873e-01, -1.6473e-03,  ..., -3.5429e+00,\n",
       "           -2.4204e+00,  7.6921e-01],\n",
       "          [ 9.7635e-01, -1.6019e+00, -1.5272e+00,  ..., -3.6681e+00,\n",
       "           -2.1308e+00, -3.5332e-01],\n",
       "          ...,\n",
       "          [-4.9001e+00, -6.8408e-01,  2.2971e+00,  ..., -2.6471e+00,\n",
       "           -1.6267e+00, -2.5713e+00],\n",
       "          [ 3.3072e+00, -1.8667e+00,  9.6693e-01,  ..., -3.8485e+00,\n",
       "           -2.2278e+00,  4.2391e+00],\n",
       "          [ 4.7925e+00,  9.2386e-01,  6.7663e-01,  ..., -3.5959e+00,\n",
       "           -2.2729e+00, -1.4680e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 2.8079, -2.3701, -5.8258,  ..., -1.8050, -0.5908, -2.6302],\n",
       "          [ 0.0696,  0.1992, -1.8310,  ...,  0.4850, -1.1207, -0.0190],\n",
       "          [-1.1992,  1.2339, -0.2952,  ...,  0.0509, -0.3867,  0.8327],\n",
       "          ...,\n",
       "          [-0.5735,  0.7185,  0.2863,  ..., -0.2146, -0.2171,  0.3128],\n",
       "          [-2.5693,  0.7370, -0.7129,  ..., -1.8785, -0.4749,  1.9457],\n",
       "          [ 1.2726,  0.2249,  1.6548,  ...,  2.2868, -0.2647,  0.5699]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-2.4365e+00, -3.7411e-01, -3.1553e-01,  ...,  3.7176e-01,\n",
       "           -1.1919e-01, -7.2125e-01],\n",
       "          [-9.0377e-01, -7.9673e-01,  1.6743e-01,  ...,  3.6984e-01,\n",
       "            3.8874e-03,  5.7858e-01],\n",
       "          [ 1.8312e+00, -9.9708e-01, -3.7254e-01,  ...,  3.4867e-01,\n",
       "           -2.0183e-03,  8.6277e-01],\n",
       "          ...,\n",
       "          [-1.2581e+00,  6.3097e-02, -5.9841e-01,  ...,  7.2434e-01,\n",
       "            1.8666e-01,  1.6025e+00],\n",
       "          [-4.3840e+00,  1.9575e+00,  1.9638e+00,  ...,  1.2946e+00,\n",
       "           -4.4748e-01,  1.4779e+00],\n",
       "          [-1.4932e+00,  1.9512e-02,  8.0153e-01,  ...,  2.1200e+00,\n",
       "           -1.5531e+00, -3.6109e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-0.2888,  0.3239,  0.1825,  ..., -0.3939, -0.4335, -0.4524],\n",
       "          [-0.4083, -0.3390, -0.0561,  ..., -0.3296, -0.0758,  0.2232],\n",
       "          [-0.1558, -0.3078, -0.0790,  ..., -0.0748, -0.0581, -0.0393],\n",
       "          ...,\n",
       "          [ 0.0753,  0.9411,  0.1079,  ...,  0.1287, -0.3297,  0.4447],\n",
       "          [-0.2691,  0.1840, -0.2811,  ..., -0.9053,  1.7921,  1.4656],\n",
       "          [ 1.6948,  0.4632, -1.7790,  ..., -1.8172, -0.2407, -0.1856]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.7204, -0.6902, -2.4884,  ..., -1.9409,  1.6568,  1.9234],\n",
       "          [ 3.6612, -2.9195, -0.4348,  ..., -1.1131,  1.0768,  1.2264],\n",
       "          [ 2.9080, -2.3109,  1.4025,  ..., -1.6859,  1.2935,  1.2761],\n",
       "          ...,\n",
       "          [-1.5496,  0.6148,  0.4859,  ..., -1.8612,  2.0464, -0.0122],\n",
       "          [ 0.1261,  1.7452, -0.3633,  ..., -0.6393,  1.7091,  4.4922],\n",
       "          [ 1.9662,  2.6661, -1.0067,  ..., -1.0376,  1.2214,  0.3404]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.2830, -0.2936, -0.8845,  ..., -0.3516, -0.6588,  0.2279],\n",
       "          [-0.2323, -0.2016, -0.1640,  ...,  0.3645, -0.2538, -0.2476],\n",
       "          [ 0.0146, -0.1913, -0.3051,  ...,  0.4211, -0.3507, -0.3211],\n",
       "          ...,\n",
       "          [ 0.2795,  0.6849, -0.2676,  ...,  0.6583,  0.1426,  0.5474],\n",
       "          [ 1.9100,  1.0995,  0.5044,  ...,  0.1510, -0.6844, -0.4330],\n",
       "          [-1.5908,  0.1923,  0.0506,  ...,  0.7113, -0.3733,  0.8022]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7035, -2.4832, -0.3223,  ..., -0.5184, -2.4600, -0.1240],\n",
       "          [-3.5948, -0.5294, -1.6635,  ..., -0.2259, -0.7361, -2.5078],\n",
       "          [-1.5112,  0.9289, -1.0459,  ..., -0.1319, -1.1478, -3.1821],\n",
       "          ...,\n",
       "          [ 1.3762, -0.1873,  0.4751,  ..., -0.6710, -2.8175, -2.9452],\n",
       "          [-0.9181,  2.6680, -1.6271,  ...,  0.9888, -1.2843, -3.2180],\n",
       "          [-2.6065,  0.2187, -1.1993,  ..., -1.8763, -2.8796, -3.4758]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.6088,  0.1791, -0.8242,  ...,  0.6676, -0.2145, -0.8332],\n",
       "          [-0.1087,  0.3149, -0.5863,  ..., -0.1492,  0.4189, -0.1376],\n",
       "          [-0.1189,  0.2576, -0.4297,  ..., -0.3083,  0.3901, -0.2183],\n",
       "          ...,\n",
       "          [ 0.0184,  1.0074, -0.4348,  ...,  0.0848,  0.4291,  0.0754],\n",
       "          [-0.2127,  0.3598,  0.3908,  ..., -0.8724, -0.6062,  0.6918],\n",
       "          [ 0.6307, -0.5022, -0.6935,  ..., -0.3224, -0.2289,  0.3761]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.1735,  1.2989,  0.3754,  ...,  2.6355, -2.9516,  1.9860],\n",
       "          [ 3.2074,  1.5241,  0.9748,  ...,  2.2178, -4.8599,  0.8753],\n",
       "          [ 1.0454,  0.8857,  1.3112,  ...,  2.3839, -5.0981,  1.6181],\n",
       "          ...,\n",
       "          [-2.1985, -1.1108, -0.3859,  ...,  3.4238, -3.3370,  3.7366],\n",
       "          [ 1.3826, -2.0208, -0.3301,  ...,  1.9907, -5.9608,  0.5726],\n",
       "          [ 5.4278, -2.6949,  2.0122,  ...,  2.5403, -3.1556,  3.6845]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.4844, -0.4986, -0.0597,  ..., -0.1327, -0.3807, -0.2893],\n",
       "          [-0.3075, -0.1740, -0.2965,  ...,  0.4621,  0.0309, -0.1660],\n",
       "          [-0.2545, -0.0144, -0.2201,  ...,  0.4919, -0.0629, -0.1533],\n",
       "          ...,\n",
       "          [-0.4000, -0.4130,  0.2200,  ...,  0.1847, -0.4974,  0.0448],\n",
       "          [-0.0737, -0.7194, -0.3538,  ..., -0.0739,  0.6864, -0.5286],\n",
       "          [ 0.0626, -0.2281, -0.4829,  ...,  0.2946,  0.4365,  0.6072]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.9257,  0.6475,  0.1888,  ...,  0.1099,  0.5005, -5.4695],\n",
       "          [ 1.3016,  0.4122,  0.7026,  ..., -0.3479, -0.8300, -2.3255],\n",
       "          [ 0.2179, -0.1099,  0.7110,  ..., -0.8683, -1.3381, -2.0316],\n",
       "          ...,\n",
       "          [-0.3989, -1.5704, -0.7064,  ..., -0.8992,  0.0070,  0.3669],\n",
       "          [ 0.2602, -0.5904, -0.2443,  ..., -0.0981,  1.3106, -2.5216],\n",
       "          [ 2.1884, -2.0453, -0.0432,  ...,  1.0834,  2.3236, -0.7923]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.2408,  0.1956, -0.6964,  ...,  0.5401,  0.4101,  0.0112],\n",
       "          [ 0.2982, -0.3206,  0.0337,  ...,  0.4676, -0.4178,  0.2551],\n",
       "          [ 0.3443, -0.6179, -0.0698,  ...,  0.2761, -0.2126,  0.6419],\n",
       "          ...,\n",
       "          [ 0.2914, -0.9330, -0.1026,  ...,  0.0180,  0.1679, -0.1173],\n",
       "          [ 0.1696, -0.7257,  0.2214,  ..., -0.2707,  0.0334, -0.1536],\n",
       "          [ 0.1834, -2.0430, -0.0763,  ...,  0.2473,  0.4889,  0.6923]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-1.4837,  0.2045, -0.6876,  ..., -2.0539, -1.0660,  0.8183],\n",
       "          [-1.9383, -0.4953, -0.3552,  ..., -1.7977, -0.4964,  1.5566],\n",
       "          [-0.5790, -1.3139,  0.4941,  ..., -1.6451,  0.2188,  1.8173],\n",
       "          ...,\n",
       "          [ 1.4117, -0.6329, -1.6064,  ..., -0.8018,  1.9610,  3.0790],\n",
       "          [ 0.9618, -0.4572, -0.9260,  ..., -1.4069,  2.5693,  0.9864],\n",
       "          [-2.0653,  0.9145, -1.0067,  ..., -1.0453,  1.8665,  2.5553]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.3809,  1.4942,  1.4052,  ..., -0.6736,  0.7431,  0.5398],\n",
       "          [-0.1320,  0.3497, -0.3179,  ...,  0.9585,  0.7277, -0.2971],\n",
       "          [ 0.1286,  0.1815, -0.3045,  ...,  0.8958,  0.4467, -0.5538],\n",
       "          ...,\n",
       "          [ 0.0805,  0.7373,  0.3759,  ..., -0.4559,  0.2039, -0.7493],\n",
       "          [ 0.4377,  0.7958,  1.4236,  ..., -0.0666, -0.4296, -0.1853],\n",
       "          [-0.8693, -1.0015,  0.6003,  ..., -0.4161, -0.2232,  0.3070]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.4473,  0.2834, -0.1233,  ...,  1.8929, -0.1135, -3.2325],\n",
       "          [ 1.0988,  0.3583, -0.4896,  ...,  0.3454, -3.1624,  1.2879],\n",
       "          [ 1.7958,  0.0108,  0.0743,  ..., -0.1367, -3.7128,  1.5283],\n",
       "          ...,\n",
       "          [-0.8739,  1.1540,  1.5786,  ...,  1.4268, -2.2023,  0.4079],\n",
       "          [-3.6623, -1.7508, -2.4920,  ..., -2.1126, -1.8342,  1.8634],\n",
       "          [-1.1554,  1.0305, -1.1199,  ..., -2.1141, -2.4228,  1.4539]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.5895,  0.7929,  0.4615,  ...,  1.2815, -0.5077,  0.2498],\n",
       "          [ 0.2299,  0.3455,  0.6391,  ...,  0.2119,  0.1337, -0.2213],\n",
       "          [ 0.3339,  0.4090,  0.7548,  ...,  0.1735,  0.2013, -0.3851],\n",
       "          ...,\n",
       "          [ 0.4202,  0.0884,  0.4702,  ..., -0.0303,  0.2425,  0.1718],\n",
       "          [-0.2383, -0.2349,  0.0148,  ..., -0.2159,  0.6186,  0.6754],\n",
       "          [ 1.6450,  0.2890,  0.0018,  ...,  0.1987, -0.1396,  0.7696]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.3948, -0.7710,  0.3992,  ..., -0.7935, -0.5857, -0.9449],\n",
       "          [-0.1743, -1.2587,  1.0807,  ..., -0.5913,  0.1731, -2.2077],\n",
       "          [-0.1151, -1.0544,  0.6339,  ..., -0.4277,  0.1276, -1.0958],\n",
       "          ...,\n",
       "          [ 0.1733, -1.7137, -0.1549,  ..., -1.4466, -1.1708, -0.7766],\n",
       "          [-0.4736, -0.1680, -2.2120,  ..., -4.1334, -0.2675,  0.8861],\n",
       "          [ 2.2916,  1.3337, -0.1713,  ..., -0.9866, -0.4404,  2.2883]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.0571,  1.5989,  0.7388,  ..., -0.2423, -1.2236,  0.0599],\n",
       "          [ 0.0804, -0.1095, -0.5821,  ...,  0.2293,  0.2354, -0.8967],\n",
       "          [ 0.0250,  0.0208, -0.4787,  ...,  0.1650,  0.1997, -0.8525],\n",
       "          ...,\n",
       "          [ 0.0335,  0.2498,  0.0163,  ..., -0.1774, -0.0072,  0.1228],\n",
       "          [-0.2092, -0.2491, -0.2753,  ...,  0.9005,  0.3687,  0.2523],\n",
       "          [-0.2549, -0.1090, -0.0818,  ..., -0.1215,  0.2813,  0.0888]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4325, -0.4618, -0.1835,  ..., -0.6444, -1.0920, -0.1601],\n",
       "          [ 1.1340, -0.9987, -1.5150,  ...,  1.1496, -0.0977,  1.0154],\n",
       "          [-1.9739, -1.1047, -1.8451,  ...,  1.4338,  0.4625,  1.2183],\n",
       "          ...,\n",
       "          [-0.5631,  0.0300,  2.1644,  ...,  2.5708, -0.0407,  0.6665],\n",
       "          [ 4.6718, -0.2412,  0.7111,  ...,  1.6406,  0.4282,  0.7524],\n",
       "          [ 3.7751,  1.9729, -0.9981,  ...,  2.8896,  0.0776,  1.8253]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.2363, -0.4247,  0.4024,  ..., -0.0869, -0.3347,  0.8899],\n",
       "          [ 0.9358,  0.1368, -0.5534,  ...,  0.4989,  0.2832,  0.3537],\n",
       "          [ 0.8417,  0.0666, -0.4778,  ...,  0.3147,  0.1465,  0.4596],\n",
       "          ...,\n",
       "          [ 3.4373,  0.0352,  0.1200,  ...,  0.9413,  1.0438, -0.2916],\n",
       "          [-0.4515, -0.0174, -0.5463,  ..., -0.3339,  0.2043,  0.4309],\n",
       "          [-0.5360, -0.1966,  0.2892,  ..., -0.4956,  0.2961,  0.4421]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.9123,  1.3960,  0.8627,  ..., -2.0544,  4.3273,  2.9414],\n",
       "          [ 2.5148,  0.4189,  0.8077,  ...,  2.7633, -1.0567,  0.0859],\n",
       "          [ 1.9961, -0.2616,  0.4608,  ...,  2.5235, -1.1496, -0.3906],\n",
       "          ...,\n",
       "          [-4.3264, -0.9806, -1.4047,  ..., -0.4930,  0.3203,  1.3135],\n",
       "          [-2.0136, -2.0326,  1.0819,  ...,  2.1494,  0.5832, -2.4070],\n",
       "          [ 3.5123, -2.1061,  1.7248,  ..., -0.2806,  0.4744, -1.1431]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.8113,  1.0940, -0.3434,  ...,  0.3900,  0.3292,  1.2926],\n",
       "          [ 0.5905,  0.4436,  0.7023,  ...,  0.0113,  0.7002, -0.4577],\n",
       "          [ 0.5820,  0.5346,  0.5255,  ...,  0.0336,  0.5808, -0.5420],\n",
       "          ...,\n",
       "          [ 0.4315, -0.2054,  0.3911,  ..., -1.0151,  0.5391,  0.0134],\n",
       "          [ 0.2742,  0.2907, -0.5165,  ..., -0.7352,  0.6555, -0.0531],\n",
       "          [-0.1731, -1.2765,  0.0782,  ...,  0.0269, -0.3570,  0.1926]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.4766,  0.4577, -0.3265,  ...,  0.2116,  0.9399, -0.6435],\n",
       "          [-1.1601,  0.5985, -0.0410,  ..., -0.1423, -0.0489, -1.4584],\n",
       "          [-0.6356,  0.1065,  0.3161,  ..., -0.2314,  0.1023, -1.2721],\n",
       "          ...,\n",
       "          [ 1.5466,  0.2035,  1.1578,  ...,  1.6800,  1.9212, -1.4647],\n",
       "          [ 0.9183,  1.1725, -0.7301,  ..., -0.4727,  0.5977, -0.3207],\n",
       "          [ 0.2550, -0.9998,  0.0417,  ...,  2.2247,  0.2038, -0.8026]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.8778, -0.5694, -2.0689,  ...,  1.2124,  0.3837,  0.3669],\n",
       "          [-0.1905, -0.4063, -1.0079,  ...,  0.4909, -0.7108,  1.0320],\n",
       "          [ 0.5174,  0.0176, -0.8005,  ...,  0.3817, -0.5455,  1.3719],\n",
       "          ...,\n",
       "          [ 0.1820, -0.2752, -0.0403,  ...,  1.2068, -0.5105,  0.2255],\n",
       "          [-0.3054, -0.1615,  0.2740,  ...,  0.3041, -1.6710,  0.1100],\n",
       "          [ 0.6678,  0.0100,  0.3404,  ..., -0.2509, -0.7867, -1.0286]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.5022, -0.8843,  0.1954,  ...,  2.0746,  6.3517,  0.4568],\n",
       "          [ 0.5272, -0.0171,  0.5105,  ..., -0.8692,  0.7758,  1.8601],\n",
       "          [ 0.3567,  0.2825,  0.3476,  ..., -1.6198, -0.2364,  1.9151],\n",
       "          ...,\n",
       "          [-1.1838,  1.4121, -1.3625,  ..., -1.1644, -0.1735, -2.5729],\n",
       "          [-2.1330,  0.8285, -0.7574,  ...,  3.3430, -2.1802, -2.0896],\n",
       "          [ 0.6306,  0.6688,  1.1965,  ...,  0.8598, -1.4977, -2.1749]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 1.0817, -0.2767,  3.1156,  ...,  0.0290, -0.2571,  1.5110],\n",
       "          [-1.7279, -0.2155, -1.2926,  ..., -0.1815,  0.2205, -1.2826],\n",
       "          [-1.5269, -0.1260, -1.1588,  ..., -0.1522, -0.0591, -0.9434],\n",
       "          ...,\n",
       "          [ 0.5456,  0.3265,  0.1377,  ...,  0.5344,  0.3090,  0.3767],\n",
       "          [-2.1617, -0.2212, -0.2115,  ..., -0.8290,  0.3785,  0.3501],\n",
       "          [-0.2815, -0.0104,  0.2738,  ..., -0.6375,  0.5069,  0.5856]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.9358,  0.8887,  0.3048,  ..., -1.6103,  3.9246, 10.9774],\n",
       "          [ 1.5791,  0.2548,  1.0875,  ..., -0.7455, -1.9800,  3.8935],\n",
       "          [ 0.4892, -0.2655,  0.3553,  ..., -0.7613, -1.9272,  1.5511],\n",
       "          ...,\n",
       "          [-1.4787, -1.1607, -0.5557,  ...,  0.1450,  0.5125,  1.2910],\n",
       "          [ 1.0977,  0.5313,  1.8884,  ..., -0.5134, -3.3961, -1.1066],\n",
       "          [ 3.0845, -0.0424,  1.8092,  ...,  0.3711,  0.4199, -0.2244]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-2.3917e+00, -3.9219e-01,  1.0431e+00,  ..., -3.9070e-01,\n",
       "            1.6710e+00, -1.4052e+00],\n",
       "          [ 5.1568e-01,  2.3881e+00, -8.0019e-01,  ...,  1.0830e+00,\n",
       "            3.3196e-01,  6.1403e-01],\n",
       "          [ 2.0094e-01,  2.1676e+00, -3.9830e-01,  ...,  1.3472e+00,\n",
       "            4.7079e-01,  5.8665e-01],\n",
       "          ...,\n",
       "          [-9.6051e-01, -1.3740e+00,  4.1569e-01,  ...,  2.1659e-01,\n",
       "            1.2972e+00,  9.8902e-03],\n",
       "          [-6.3642e-01, -2.4652e-01, -1.6141e+00,  ..., -2.5810e-01,\n",
       "           -5.5492e-05, -5.7129e-01],\n",
       "          [ 7.3823e-01, -3.5097e-01, -7.2466e-02,  ...,  6.5428e-01,\n",
       "            1.5011e+00,  1.9839e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.2874,  1.2532, -0.7242,  ...,  0.5194, -0.3780,  3.3769],\n",
       "          [ 1.1209, -0.8448,  0.2498,  ...,  0.1283,  2.3777,  1.2378],\n",
       "          [ 0.5374, -0.6355,  0.1527,  ..., -0.2344,  1.8118,  0.4347],\n",
       "          ...,\n",
       "          [-1.6092, -0.6462, -0.3883,  ...,  0.0593,  1.8408,  0.8052],\n",
       "          [-0.5803,  0.7664, -0.2830,  ...,  3.1789,  1.8017,  1.7682],\n",
       "          [ 0.7498,  0.7314, -0.7526,  ..., -0.4957,  1.3883, -0.4626]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-1.1943e+00, -1.4423e-01,  5.4868e+00,  ...,  1.9181e+00,\n",
       "           -6.6254e-01, -2.7780e-02],\n",
       "          [ 7.0350e-02,  1.1677e+00,  1.1763e+00,  ..., -5.5605e-01,\n",
       "            2.3627e+00,  2.4097e-01],\n",
       "          [-1.1437e-01,  6.8783e-01,  1.5193e+00,  ..., -2.5370e-01,\n",
       "            2.3598e+00,  6.0902e-01],\n",
       "          ...,\n",
       "          [-4.6222e-01, -3.3216e-03, -4.3698e-01,  ...,  5.1953e-01,\n",
       "           -4.4101e-01, -4.7690e-01],\n",
       "          [-1.3162e+00, -5.9197e-01, -6.8146e+00,  ...,  3.7784e+00,\n",
       "            3.2649e+00, -4.8503e+00],\n",
       "          [-2.7252e+00,  2.7309e+00,  2.3611e+00,  ...,  2.1233e-01,\n",
       "           -8.0633e-01,  6.8272e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.1413,  0.3225, -0.0103,  ..., -0.8148, -0.6490,  2.5024],\n",
       "          [ 0.1574, -0.1472,  0.4943,  ...,  1.1365, -0.3583, -0.0186],\n",
       "          [ 0.4542, -0.2839,  0.4689,  ...,  0.9580, -0.8042,  0.4397],\n",
       "          ...,\n",
       "          [-1.2163, -0.2280, -2.1919,  ...,  1.0764, -0.9912, -0.2375],\n",
       "          [-0.1304,  0.1640,  0.0399,  ...,  2.0289,  0.0785, -0.4430],\n",
       "          [-0.0798,  1.1372,  0.2142,  ...,  0.6862, -1.7610, -0.6300]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.6289, -0.5716, -2.9922,  ...,  3.6869,  1.8709, -2.9562],\n",
       "          [-1.1754,  1.0996,  0.9988,  ...,  0.3801,  2.3168, -0.0680],\n",
       "          [-1.1162,  1.4620,  2.4048,  ...,  0.1501,  1.9535, -0.1915],\n",
       "          ...,\n",
       "          [ 0.4079,  1.5052,  0.0280,  ...,  0.3646,  1.7156, -0.6651],\n",
       "          [-2.7263,  1.2985, -2.3989,  ..., -2.3535,  4.3459, -3.9985],\n",
       "          [-0.4861,  2.6891,  0.1770,  ...,  0.3090,  1.4702,  1.2048]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.3825,  0.0336,  0.4236,  ..., -0.0891,  0.9931, -3.6406],\n",
       "          [-0.5118,  0.1651,  0.8312,  ..., -3.6232,  1.4958, -0.3093],\n",
       "          [-0.0680,  0.3152,  0.5897,  ..., -4.8818,  1.0993, -1.4178],\n",
       "          ...,\n",
       "          [ 1.5247,  1.0951, -1.0349,  ..., -0.4820,  1.0052, -1.0148],\n",
       "          [-0.7301, -0.0500, -0.1936,  ..., -1.6880, -0.7870, -0.0220],\n",
       "          [-1.2003, -0.3267,  0.2174,  ..., -0.1575,  1.7342, -0.5402]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-2.7588,  2.8471,  0.5303,  ..., -2.1065, -3.3846, -2.7491],\n",
       "          [ 1.4253, -0.2420, -0.0584,  ..., -0.3422, -3.0433,  0.7456],\n",
       "          [-1.4730,  0.9856,  0.6514,  ..., -0.5979, -3.1118,  0.8809],\n",
       "          ...,\n",
       "          [-0.5572,  2.6002,  1.9724,  ..., -0.8355,  0.0314, -0.6643],\n",
       "          [ 0.7950,  0.2061,  0.4858,  ...,  2.6874, -2.7799, -1.5837],\n",
       "          [ 0.1651,  0.1444,  1.0633,  ..., -0.3639, -0.7180, -0.2103]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0096,  0.6748,  0.8011,  ...,  1.0971,  2.0595, -0.5275],\n",
       "          [ 0.7212, -1.0256,  0.4116,  ..., -0.7511,  1.0003, -3.8538],\n",
       "          [-1.2939, -1.2439,  0.4107,  ..., -0.6351,  0.8887, -3.4330],\n",
       "          ...,\n",
       "          [ 0.0248, -0.3696, -0.6489,  ...,  1.0364,  2.1725, -2.8268],\n",
       "          [ 1.7152,  0.8492,  0.1413,  ..., -0.5548,  1.8047, -1.4110],\n",
       "          [ 1.8085,  1.1364, -0.0813,  ..., -0.5361,  1.3019, -1.7981]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 2.5489,  0.0124, -1.4080,  ..., -3.4707,  0.6342,  2.4149],\n",
       "          [-0.4636,  0.8600, -1.9565,  ..., -1.1407, -0.8028,  2.3704],\n",
       "          [-0.1667,  0.5459, -1.5189,  ..., -0.7734,  0.1891,  2.1013],\n",
       "          ...,\n",
       "          [-0.1568,  0.5913, -0.3691,  ..., -0.8941,  0.1604,  1.3874],\n",
       "          [ 0.3948,  1.6004, -0.8502,  ..., -0.0625, -1.8039,  1.3769],\n",
       "          [ 0.7765,  1.0110, -0.1520,  ..., -0.6785, -0.4750,  1.0532]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None, image_hidden_states=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663b7a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[257152, 257152, 257152,  ...,    108,   4991,      1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[[[-0.9939, -0.9947, -0.9964,  ..., -0.9976, -0.9985, -0.9990],\n",
       "          [-0.9933, -0.9941, -0.9958,  ..., -0.9974, -0.9987, -0.9993],\n",
       "          [-0.9922, -0.9928, -0.9943,  ..., -0.9970, -0.9990, -0.9999],\n",
       "          ...,\n",
       "          [-0.9994, -0.9993, -0.9993,  ..., -0.9972, -0.9955, -0.9947],\n",
       "          [-0.9981, -0.9982, -0.9984,  ..., -0.9961, -0.9944, -0.9936],\n",
       "          [-0.9974, -0.9976, -0.9980,  ..., -0.9956, -0.9940, -0.9931]],\n",
       "\n",
       "         [[-0.9952, -0.9962, -0.9985,  ..., -0.9966, -0.9945, -0.9935],\n",
       "          [-0.9957, -0.9965, -0.9983,  ..., -0.9964, -0.9952, -0.9947],\n",
       "          [-0.9968, -0.9972, -0.9980,  ..., -0.9958, -0.9968, -0.9972],\n",
       "          ...,\n",
       "          [-0.9987, -0.9977, -0.9945,  ..., -0.9949, -0.9965, -0.9972],\n",
       "          [-0.9945, -0.9954, -0.9970,  ..., -0.9948, -0.9958, -0.9963],\n",
       "          [-0.9925, -0.9943, -0.9983,  ..., -0.9948, -0.9955, -0.9959]],\n",
       "\n",
       "         [[-0.9984, -0.9977, -0.9963,  ..., -0.9933, -0.9922, -0.9922],\n",
       "          [-0.9986, -0.9978, -0.9961,  ..., -0.9939, -0.9930, -0.9927],\n",
       "          [-0.9990, -0.9979, -0.9955,  ..., -0.9952, -0.9947, -0.9946],\n",
       "          ...,\n",
       "          [-0.9979, -0.9978, -0.9976,  ..., -0.9963, -0.9977, -0.9983],\n",
       "          [-0.9942, -0.9950, -0.9968,  ..., -0.9944, -0.9970, -0.9982],\n",
       "          [-0.9924, -0.9937, -0.9964,  ..., -0.9936, -0.9967, -0.9982]]]]), 'labels': tensor([[-100, -100, -100,  ..., -100, 4991,    1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_proc = model.processor(text=samples['text_input'], images=samples['image'], suffix=samples['answer'], return_tensors=\"pt\")\n",
    "all_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f152681d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[257152, 257152, 257152,  ...,    108,   4991,      1],\n",
       "        [257152, 257152, 257152,  ...,    108,   4991,      1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 1, 1],\n",
       "        [0, 0, 0,  ..., 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[[[-0.9939, -0.9947, -0.9964,  ..., -0.9976, -0.9985, -0.9990],\n",
       "          [-0.9933, -0.9941, -0.9958,  ..., -0.9974, -0.9987, -0.9993],\n",
       "          [-0.9922, -0.9928, -0.9943,  ..., -0.9970, -0.9990, -0.9999],\n",
       "          ...,\n",
       "          [-0.9994, -0.9993, -0.9993,  ..., -0.9972, -0.9955, -0.9947],\n",
       "          [-0.9981, -0.9982, -0.9984,  ..., -0.9961, -0.9944, -0.9936],\n",
       "          [-0.9974, -0.9976, -0.9980,  ..., -0.9956, -0.9940, -0.9931]],\n",
       "\n",
       "         [[-0.9952, -0.9962, -0.9985,  ..., -0.9966, -0.9945, -0.9935],\n",
       "          [-0.9957, -0.9965, -0.9983,  ..., -0.9964, -0.9952, -0.9947],\n",
       "          [-0.9968, -0.9972, -0.9980,  ..., -0.9958, -0.9968, -0.9972],\n",
       "          ...,\n",
       "          [-0.9987, -0.9977, -0.9945,  ..., -0.9949, -0.9965, -0.9972],\n",
       "          [-0.9945, -0.9954, -0.9970,  ..., -0.9948, -0.9958, -0.9963],\n",
       "          [-0.9925, -0.9943, -0.9983,  ..., -0.9948, -0.9955, -0.9959]],\n",
       "\n",
       "         [[-0.9984, -0.9977, -0.9963,  ..., -0.9933, -0.9922, -0.9922],\n",
       "          [-0.9986, -0.9978, -0.9961,  ..., -0.9939, -0.9930, -0.9927],\n",
       "          [-0.9990, -0.9979, -0.9955,  ..., -0.9952, -0.9947, -0.9946],\n",
       "          ...,\n",
       "          [-0.9979, -0.9978, -0.9976,  ..., -0.9963, -0.9977, -0.9983],\n",
       "          [-0.9942, -0.9950, -0.9968,  ..., -0.9944, -0.9970, -0.9982],\n",
       "          [-0.9924, -0.9937, -0.9964,  ..., -0.9936, -0.9967, -0.9982]]],\n",
       "\n",
       "\n",
       "        [[[-0.9939, -0.9947, -0.9964,  ..., -0.9976, -0.9985, -0.9990],\n",
       "          [-0.9933, -0.9941, -0.9958,  ..., -0.9974, -0.9987, -0.9993],\n",
       "          [-0.9922, -0.9928, -0.9943,  ..., -0.9970, -0.9990, -0.9999],\n",
       "          ...,\n",
       "          [-0.9994, -0.9993, -0.9993,  ..., -0.9972, -0.9955, -0.9947],\n",
       "          [-0.9981, -0.9982, -0.9984,  ..., -0.9961, -0.9944, -0.9936],\n",
       "          [-0.9974, -0.9976, -0.9980,  ..., -0.9956, -0.9940, -0.9931]],\n",
       "\n",
       "         [[-0.9952, -0.9962, -0.9985,  ..., -0.9966, -0.9945, -0.9935],\n",
       "          [-0.9957, -0.9965, -0.9983,  ..., -0.9964, -0.9952, -0.9947],\n",
       "          [-0.9968, -0.9972, -0.9980,  ..., -0.9958, -0.9968, -0.9972],\n",
       "          ...,\n",
       "          [-0.9987, -0.9977, -0.9945,  ..., -0.9949, -0.9965, -0.9972],\n",
       "          [-0.9945, -0.9954, -0.9970,  ..., -0.9948, -0.9958, -0.9963],\n",
       "          [-0.9925, -0.9943, -0.9983,  ..., -0.9948, -0.9955, -0.9959]],\n",
       "\n",
       "         [[-0.9984, -0.9977, -0.9963,  ..., -0.9933, -0.9922, -0.9922],\n",
       "          [-0.9986, -0.9978, -0.9961,  ..., -0.9939, -0.9930, -0.9927],\n",
       "          [-0.9990, -0.9979, -0.9955,  ..., -0.9952, -0.9947, -0.9946],\n",
       "          ...,\n",
       "          [-0.9979, -0.9978, -0.9976,  ..., -0.9963, -0.9977, -0.9983],\n",
       "          [-0.9942, -0.9950, -0.9968,  ..., -0.9944, -0.9970, -0.9982],\n",
       "          [-0.9924, -0.9937, -0.9964,  ..., -0.9936, -0.9967, -0.9982]]]]), 'labels': tensor([[-100, -100, -100,  ..., -100, 4991,    1],\n",
       "        [-100, -100, -100,  ..., -100, 4991,    1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.processor(text=samples['text_input']*2, images=torch.stack([image_rand[0], image_rand[0]]), suffix=samples['answer']*2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad8ea7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[257152, 257152, 257152,  ...,    736, 235336,    108]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[[[-0.9939, -0.9947, -0.9964,  ..., -0.9976, -0.9985, -0.9990],\n",
       "          [-0.9933, -0.9941, -0.9958,  ..., -0.9974, -0.9987, -0.9993],\n",
       "          [-0.9922, -0.9928, -0.9943,  ..., -0.9970, -0.9990, -0.9999],\n",
       "          ...,\n",
       "          [-0.9994, -0.9993, -0.9993,  ..., -0.9972, -0.9955, -0.9947],\n",
       "          [-0.9981, -0.9982, -0.9984,  ..., -0.9961, -0.9944, -0.9936],\n",
       "          [-0.9974, -0.9976, -0.9980,  ..., -0.9956, -0.9940, -0.9931]],\n",
       "\n",
       "         [[-0.9952, -0.9962, -0.9985,  ..., -0.9966, -0.9945, -0.9935],\n",
       "          [-0.9957, -0.9965, -0.9983,  ..., -0.9964, -0.9952, -0.9947],\n",
       "          [-0.9968, -0.9972, -0.9980,  ..., -0.9958, -0.9968, -0.9972],\n",
       "          ...,\n",
       "          [-0.9987, -0.9977, -0.9945,  ..., -0.9949, -0.9965, -0.9972],\n",
       "          [-0.9945, -0.9954, -0.9970,  ..., -0.9948, -0.9958, -0.9963],\n",
       "          [-0.9925, -0.9943, -0.9983,  ..., -0.9948, -0.9955, -0.9959]],\n",
       "\n",
       "         [[-0.9984, -0.9977, -0.9963,  ..., -0.9933, -0.9922, -0.9922],\n",
       "          [-0.9986, -0.9978, -0.9961,  ..., -0.9939, -0.9930, -0.9927],\n",
       "          [-0.9990, -0.9979, -0.9955,  ..., -0.9952, -0.9947, -0.9946],\n",
       "          ...,\n",
       "          [-0.9979, -0.9978, -0.9976,  ..., -0.9963, -0.9977, -0.9983],\n",
       "          [-0.9942, -0.9950, -0.9968,  ..., -0.9944, -0.9970, -0.9982],\n",
       "          [-0.9924, -0.9937, -0.9964,  ..., -0.9936, -0.9967, -0.9982]]]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.processor(text=samples['text_input'], images=samples['image'], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59e5914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e4a940c2b74f4f8bfccee212b8d6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(3.7783, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 64, 64)\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5, 0.5, 1.0]),\n",
    "    \"n_answers\": torch.tensor([2, 1]),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"paligemma_vqa\", model_type=\"paligemma-3b-ft-vqav2-448\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6e842f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20b91b50f8946749299930cfb5a08c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.97 GiB. GPU 0 has a total capacity of 44.38 GiB of which 2.47 GiB is free. Including non-PyTorch memory, this process has 41.91 GiB memory in use. Of the allocated memory 39.80 GiB is allocated by PyTorch, and 532.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m samples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_rand,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is this?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is that?\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miters\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_iters_per_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaligemma_vqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaligemma-3b-ft-vqav2-448\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/coc/pskynet4/chuang475/projects/vlm_robustness/model/paligemma_vqa.py:67\u001b[0m, in \u001b[0;36mPaliGemma_VQA.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     64\u001b[0m     questions_stack \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_input\u001b[39m\u001b[38;5;124m\"\u001b[39m][b]] \u001b[38;5;241m*\u001b[39m n\n\u001b[1;32m     66\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(text\u001b[38;5;241m=\u001b[39mquestions_stack, images\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(image_stack, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), suffix\u001b[38;5;241m=\u001b[39msamples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs}\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/transformers/models/paligemma/modeling_paligemma.py:471\u001b[0m, in \u001b[0;36mPaliGemmaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    469\u001b[0m             position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(attention_mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    470\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 471\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    484\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/transformers/models/gemma/modeling_gemma.py:1127\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1114\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1115\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1124\u001b[0m )\n\u001b[1;32m   1126\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1127\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1128\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1129\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/flash/miniconda3/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.97 GiB. GPU 0 has a total capacity of 44.38 GiB of which 2.47 GiB is free. Including non-PyTorch memory, this process has 41.91 GiB memory in use. Of the allocated memory 39.80 GiB is allocated by PyTorch, and 532.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 64, 64)\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\", \"dog\"],\n",
    "    \"weight\": torch.tensor([0.5, 0.5, 1.0]),\n",
    "    \"n_answers\": torch.tensor([2, 1]),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"paligemma_vqa\", model_type=\"paligemma-3b-ft-vqav2-448\").to(\"cuda:0\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9eaee3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57931b0e1e324e1f8dc778e1756f9cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(3.6914, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 64, 64)\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\"],\n",
    "    \"weight\": torch.tensor([1.0, 1.0]),\n",
    "    \"n_answers\": torch.tensor([1, 1]),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"paligemma_vqa\", model_type=\"paligemma-3b-ft-vqav2-448\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a8cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacb45c37504768a9d585fcc3cec28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacb45c37504768a9d585fcc3cec28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacb45c37504768a9d585fcc3cec28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacb45c37504768a9d585fcc3cec28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacb45c37504768a9d585fcc3cec28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacb45c37504768a9d585fcc3cec28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(3.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output.loss:  tensor(3.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output.loss:  tensor(3.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output.loss:  tensor(3.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output.loss:  tensor(3.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "output.loss:  tensor(3.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 64, 64)\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand,\n",
    "    \"text_input\": [\"What is this?\", \"What is that?\"],\n",
    "    \"answer\": [\"cat\", \"dog\"],\n",
    "    \"weight\": torch.tensor([1.0, 1.0]),\n",
    "    \"n_answers\": torch.tensor([1, 1]),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"paligemma_vqa\", model_type=\"paligemma-3b-ft-vqav2-448\").to(\"cuda:0\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7089ff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d44e52f5684f02b9fc13144c6705e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.loss:  tensor(3.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(24, True)\n",
    "\n",
    "image_rand = torch.rand(2, 3, 224, 224).to(\"cuda:0\")\n",
    "\n",
    "samples = {\n",
    "    \"image\": image_rand[[0]],\n",
    "    \"text_input\": [\"What is this?\"],\n",
    "    \"answer\": [\"cat\"],\n",
    "    \"weight\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"n_answers\": torch.tensor([1]).to(\"cuda:0\"),\n",
    "    \"epoch\": 0, \"iters\": 0, \"num_iters_per_epoch\": 1000,\n",
    "}\n",
    "\n",
    "model = load_model(\"paligemma_vqa\", model_type=\"paligemma-3b-ft-vqav2-448\").to(\"cuda:0\").eval()\n",
    "output = model(samples)\n",
    "print(\"output.loss: \", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031640fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61399c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is this?', 'What is that?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"text_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b76e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'pixel_values']),\n",
       " torch.Size([2, 1030]),\n",
       " torch.Size([2, 1030]),\n",
       " torch.Size([2, 3, 448, 448]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_txt = model.processor(text=samples[\"text_input\"], images=samples[\"image\"]).to(\"cuda:0\")\n",
    "img_txt.keys(), img_txt['input_ids'].shape, img_txt['attention_mask'].shape, img_txt['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2194795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'labels']),\n",
       " torch.Size([2, 1032]),\n",
       " torch.Size([2, 1032]),\n",
       " torch.Size([2, 3, 448, 448]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_txt_ans = model.processor(\n",
    "    text=samples[\"text_input\"], \n",
    "    images=samples[\"image\"], \n",
    "    suffix=[\"cat\", \"dog\"]\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "img_txt_ans.keys(), img_txt_ans['input_ids'].shape, img_txt_ans['attention_mask'].shape, img_txt_ans['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69091582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0,  ..., 0, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 1, 1]], device='cuda:0'),\n",
       " tensor([[ -100,  -100,  -100,  ...,  -100,  4991,     1],\n",
       "         [ -100,  -100,  -100,  ...,  -100, 12240,     1]], device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_txt_ans['token_type_ids'], img_txt_ans['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251d7f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[257152, 257152, 257152,  ...,    736, 235336,    108],\n",
       "         [257152, 257152, 257152,  ...,    674, 235336,    108]],\n",
       "        device='cuda:0'),\n",
       " tensor([[257152, 257152, 257152,  ...,    108,   4991,      1],\n",
       "         [257152, 257152, 257152,  ...,    108,  12240,      1]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_txt['input_ids'], img_txt_ans['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3cf9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_txt['attention_mask'], img_txt_ans['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c270748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(sum(sum(img_txt['pixel_values'] != img_txt_ans['pixel_values']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcba286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 14:39:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   30C    P0    72W / 300W |  39228MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2772399      C   ...da3/envs/lavis/bin/python    39226MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Tue Jul  2 14:39:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   30C    P0    72W / 300W |  39228MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2772399      C   ...da3/envs/lavis/bin/python    39226MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Tue Jul  2 14:39:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   30C    P0    72W / 300W |  39228MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2772399      C   ...da3/envs/lavis/bin/python    39226MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Tue Jul  2 14:39:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   30C    P0    72W / 300W |  39228MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2772399      C   ...da3/envs/lavis/bin/python    39226MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Tue Jul  2 14:39:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   30C    P0    72W / 300W |  39228MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2772399      C   ...da3/envs/lavis/bin/python    39226MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Tue Jul  2 14:39:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   30C    P0    72W / 300W |  39228MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2772399      C   ...da3/envs/lavis/bin/python    39226MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"cat\", \"cat\", \"dog\", \"dog\", \"dog\"]\n",
    "max(set(a), key=a.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
